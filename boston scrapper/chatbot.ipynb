{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "155d7af6-bb6f-4618-8099-e1aee45c901d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded and validated.\n",
      "Loaded 472 raw data entries from 'scraped_data.pkl'.\n",
      "Converted 472 entries into LlamaIndex Documents.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from llama_index.core import Document\n",
    "\n",
    "# --- Configuration ---\n",
    "# ✅ Step 1: Load environment variables from .env file\n",
    "# Ensure your 'main.env' file contains:\n",
    "# PINECONE_API_KEY=\"YOUR_PINECONE_API_KEY\"\n",
    "# PINECONE_ENV=\"YOUR_PINECONE_ENVIRONMENT\" # e.g., \"gcp-starter\"\n",
    "# PINECONE_INDEX=\"YOUR_PINECONE_INDEX_NAME\"\n",
    "# GOOGLE_API_KEY=\"YOUR_GOOGLE_API_KEY\"\n",
    "\n",
    "load_dotenv(\"main.env\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")\n",
    "PINECONE_INDEX = os.getenv(\"PINECONE_INDEX\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Validate environment variables are loaded\n",
    "if not all([PINECONE_API_KEY, PINECONE_ENV, PINECONE_INDEX, GOOGLE_API_KEY]):\n",
    "    raise ValueError(\n",
    "        \"One or more environment variables (PINECONE_API_KEY, PINECONE_ENV, \"\n",
    "        \"PINECONE_INDEX, GOOGLE_API_KEY) are not set. \"\n",
    "        \"Please check your 'main.env' file.\"\n",
    "    )\n",
    "print(\"Environment variables loaded and validated.\")\n",
    "\n",
    "# --- Data Loading ---\n",
    "# ✅ Step 2: Load documents from pickle file\n",
    "# This assumes 'scraped_data.pkl' contains a list of dictionaries,\n",
    "# where each dictionary has 'content', 'url', and 'title' keys.\n",
    "try:\n",
    "    with open(\"scraped_data.pkl\", \"rb\") as f:\n",
    "        raw_data = pickle.load(f)\n",
    "    print(f\"Loaded {len(raw_data)} raw data entries from 'scraped_data.pkl'.\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\n",
    "        \"scraped_data.pkl not found. Please ensure the scraped data \"\n",
    "        \"is in the correct path.\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    raise IOError(f\"Error loading scraped_data.pkl: {e}\")\n",
    "\n",
    "# Convert raw data into LlamaIndex Document objects\n",
    "documents = []\n",
    "for item in raw_data:\n",
    "    content = item.get(\"content\")\n",
    "    if content: # Only process entries with content\n",
    "        documents.append(\n",
    "            Document(\n",
    "                text=content,\n",
    "                metadata={\n",
    "                    \"url\": item.get(\"url\", \"N/A\"),\n",
    "                    \"title\": item.get(\"title\", \"N/A\")\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "print(f\"Converted {len(documents)} entries into LlamaIndex Documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1941f547-7772-4307-a3c4-30ba6feb7764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/boston_scrapper/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to existing Pinecone index: boston-chatbot\n",
      "Using embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Vector store and storage context configured.\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "from pinecone import Pinecone as PineconeClient # Import Pinecone client for explicit initialization\n",
    "\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Assuming PINECONE_API_KEY, PINECONE_ENV, PINECONE_INDEX are already loaded from previous step\n",
    "\n",
    "# --- Pinecone Setup ---\n",
    "# ✅ Step 3: Connect to Pinecone\n",
    "# Using the new Pinecone client for explicit initialization\n",
    "try:\n",
    "    pc = PineconeClient(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)\n",
    "    # Since the index is already created manually, we can directly connect to it.\n",
    "    pc_index = pc.Index(PINECONE_INDEX)\n",
    "    print(f\"Successfully connected to existing Pinecone index: {PINECONE_INDEX}\")\n",
    "\n",
    "except Exception as e:\n",
    "    raise ConnectionError(f\"Failed to connect to Pinecone: {e}\")\n",
    "\n",
    "# --- Embedding Model and Vector Store ---\n",
    "# ✅ Step 4: Setup vector store and embedding model\n",
    "vector_store = PineconeVectorStore(pinecone_index=pc_index)\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "print(f\"Using embedding model: {embed_model.model_name}\")\n",
    "print(\"Vector store and storage context configured.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f539768d-a17f-4cdc-a52f-63d03692a40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building index from documents and storing in Pinecone...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|█████████████████████████| 472/472 [00:00<00:00, 645.41it/s]\n",
      "Generating embeddings: 100%|██████████████████| 589/589 [00:20<00:00, 28.43it/s]\n",
      "Upserted vectors: 100%|███████████████████████| 589/589 [00:15<00:00, 37.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index built and stored in Pinecone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Assuming 'documents', 'storage_context', and 'embed_model' are available from previous steps\n",
    "\n",
    "# --- Index Creation/Loading ---\n",
    "# ✅ Step 5: Build or load the index\n",
    "# This part is crucial:\n",
    "#\n",
    "# Option A: Build and store the index from documents (run this if you need to index your data)\n",
    "#   - Use this if this is the FIRST time you're running this script with your 'scraped_data.pkl'\n",
    "#     and you want to upload the embeddings to Pinecone.\n",
    "#   - This step can take a significant amount of time depending on the size of your data.\n",
    "#   - Uncomment the lines below for Option A and comment out Option B.\n",
    "\n",
    "print(\"Building index from documents and storing in Pinecone...\")\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, # 'documents' from Step 2\n",
    "    storage_context=storage_context, # 'storage_context' from Step 3\n",
    "    embed_model=embed_model, # 'embed_model' from Step 3\n",
    "    show_progress=True # Show progress during embedding and upserting\n",
    ")\n",
    "print(\"Index built and stored in Pinecone.\")\n",
    "\n",
    "\n",
    "# Option B: Load existing index from Pinecone (skip embedding again if already indexed)\n",
    "#   - Use this if your documents have ALREADY been embedded and uploaded to Pinecone\n",
    "#     in a previous run (e.g., from running Option A before).\n",
    "#   - This is much faster as it doesn't re-embed your data.\n",
    "#   - Uncomment the lines below for Option B and comment out Option A.\n",
    "\n",
    "# print(\"Loading existing index from Pinecone...\")\n",
    "# index = VectorStoreIndex.from_vector_store(\n",
    "#     vector_store=vector_store, # 'vector_store' from Step 3\n",
    "#     storage_context=storage_context, # 'storage_context' from Step 3\n",
    "#     embed_model=embed_model # 'embed_model' from Step 3\n",
    "# )\n",
    "# print(\"Index loaded from Pinecone.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e60e495-06b9-4ac9-a156-e643f95638b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LLM: models/gemini-1.5-pro-latest\n",
      "LlamaIndex settings configured (LLM and Embedding Model).\n",
      "\n",
      "Query engine created.\n",
      "\n",
      "💬 Website Chatbot is ready! (type 'exit' or 'quit' to end)\n",
      "---\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  services provided\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: The Office of Research provides workshops and training on research how-tos, and resources for communicating and disseminating research findings.  They offer accommodations such as ASL interpreters and CART upon request.  Researchers can also propose topics for future events.  For assistance with sharing research, various resources and offices at BU are available to help faculty broaden their reach.\n",
      "\n",
      "\n",
      "--- Retrieved Source Chunks ---\n",
      "Chunk 1 (Score: 0.35):\n",
      "URL: https://www.bu.edu/research/events-updates-trainings/research-how-tos/\n",
      "Title: Research How-to Workshops & Trainings | Office of Research\n",
      "Text: For questions about accessibility or to request an accommodation (e.g., ASL interpreters, Communication Access Realtime Translation CART), please email research@bu.edu.   View past events, or learn mo...\n",
      "\n",
      "Chunk 2 (Score: 0.31):\n",
      "URL: https://www.bu.edu/research/communication-dissemination/\n",
      "Title: Communication & Dissemination | Office of Research\n",
      "Text: Ready to share your research with the world? Read on to learn about the resources available to you, and the offices at BU that work to help faculty extend their reach....\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what is the summary of this websit?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Boston University's financial aid website emphasizes its role as the main source of information for students seeking financial assistance.  The Human Resources site highlights employee benefits, flexible work options, and access to newsletters.\n",
      "\n",
      "\n",
      "--- Retrieved Source Chunks ---\n",
      "Chunk 1 (Score: 0.38):\n",
      "URL: https://www.bu.edu/finaid/how-aid-works/\n",
      "Title: How Aid Works | Financial Assistance\n",
      "Text: We encourage you to treat our website as your primary source of information. If you come across any stumbling blocks, we’re here with answers and guidance....\n",
      "\n",
      "Chunk 2 (Score: 0.35):\n",
      "URL: https://www.bu.edu/hr/\n",
      "Title: Human Resources\n",
      "Text: loading slideshow... BU Total Rewards & myFiTage Flexible Work at Boston University BUHR Newsletter Archive...\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👋 Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Corrected import for GoogleGenAI\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.core import VectorStoreIndex # Import VectorStoreIndex if not already imported in this cell\n",
    "\n",
    "# Assuming 'index', 'GOOGLE_API_KEY', and 'embed_model' are available from previous steps\n",
    "\n",
    "# --- LLM Configuration ---\n",
    "# ✅ Step 6: Set up Google Generative AI LLM\n",
    "llm = GoogleGenAI(\n",
    "    # The 'gemini-pro' model was not found.\n",
    "    # Using 'models/gemini-1.5-pro-latest' which was listed as available in your diagnostic output.\n",
    "    model=\"models/gemini-1.5-pro-latest\", # Changed model name to a supported version\n",
    "    api_key=GOOGLE_API_KEY,\n",
    "    temperature=0.3,            # Adjust for creativity vs. factualness\n",
    "    max_output_tokens=512       # Limit response length\n",
    ")\n",
    "print(f\"Using LLM: {llm.model}\")\n",
    "\n",
    "# ✅ Step 7: Apply LLM and Embedding Model to LlamaIndex global settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model # Ensure embedding model is also set globally\n",
    "print(\"LlamaIndex settings configured (LLM and Embedding Model).\\n\")\n",
    "\n",
    "# --- Query Engine ---\n",
    "# ✅ Step 8: Create the query engine\n",
    "# To retrieve relevant chunks along with the answer, we configure the response_mode.\n",
    "# \"tree_summarize\" is a good default that synthesizes an answer and includes source nodes.\n",
    "query_engine = index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\", # This mode helps in getting source nodes\n",
    "    # You can also explicitly set the retriever mode if needed, e.g., retriever_mode=\"embedding\"\n",
    ")\n",
    "print(\"Query engine created.\")\n",
    "\n",
    "# --- Interactive Chatbot ---\n",
    "# ✅ Step 9: Start the interactive chatbot\n",
    "print(\"\\n💬 Website Chatbot is ready! (type 'exit' or 'quit' to end)\")\n",
    "print(\"---\")\n",
    "\n",
    "while True:\n",
    "    query = input(\"You: \")\n",
    "    if query.strip().lower() in {\"exit\", \"quit\"}:\n",
    "        print(\"👋 Goodbye!\")\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        response = query_engine.query(query)\n",
    "        print(f\"Bot: {response}\\n\")\n",
    "\n",
    "        # --- Added: Print Source Chunks ---\n",
    "        if response.source_nodes:\n",
    "            print(\"--- Retrieved Source Chunks ---\")\n",
    "            for i, node in enumerate(response.source_nodes):\n",
    "                print(f\"Chunk {i+1} (Score: {node.score:.2f}):\")\n",
    "                print(f\"URL: {node.metadata.get('url', 'N/A')}\")\n",
    "                print(f\"Title: {node.metadata.get('title', 'N/A')}\")\n",
    "                print(f\"Text: {node.text[:200]}...\\n\") # Print first 200 chars of chunk text\n",
    "        # --- End Added ---\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error during query: {e}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9e1140-9eab-4529-87d7-7ca143ead243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
